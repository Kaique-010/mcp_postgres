"""
Debug simples do LLM para investigar por que est√° gerando SQL corrompido
"""

import os

def testar_llm_simples():
    """Testa o LLM diretamente sem depender do Django"""
    
    print("üîç INVESTIGANDO PROBLEMA DO LLM")
    print("=" * 50)
    
    # Verificar vari√°veis de ambiente
    openai_key = os.environ.get('OPENAI_API_KEY')
    print(f"OPENAI_API_KEY definida: {'Sim' if openai_key else 'N√£o'}")
    
    if not openai_key:
        print("‚ùå OPENAI_API_KEY n√£o definida!")
        print("   Isso pode explicar por que o LLM est√° gerando respostas corrompidas.")
        print("   O LLM pode estar usando um modelo local ou mock que gera texto aleat√≥rio.")
        return
    
    print(f"Primeiros 10 chars da key: {openai_key[:10]}...")
    
    # Tentar importar e testar LLM
    try:
        from langchain_openai import ChatOpenAI
        print("‚úÖ langchain_openai importado com sucesso")
        
        # Criar LLM
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=100
        )
        print("‚úÖ LLM criado com sucesso")
        
        # Teste simples
        print("\nüß™ TESTE SIMPLES")
        print("-" * 30)
        
        prompt_teste = "Responda apenas: SELECT 1"
        print(f"Prompt: '{prompt_teste}'")
        
        resposta = llm.invoke(prompt_teste)
        print(f"Tipo resposta: {type(resposta)}")
        
        if hasattr(resposta, 'content'):
            content = resposta.content
            print(f"Content: '{content}'")
            print(f"Length: {len(content)}")
            
            # Verificar caracteres suspeitos
            suspeitos = ['√∫', '√≥', '√£', 'subooo', 'ssso', 'uuosbsusb']
            encontrados = []
            for suspeito in suspeitos:
                if suspeito in content.lower():
                    encontrados.append(suspeito)
            
            if encontrados:
                print(f"‚ö†Ô∏è CARACTERES SUSPEITOS ENCONTRADOS: {encontrados}")
                print("   Isso confirma que o LLM est√° gerando texto corrompido!")
            else:
                print("‚úÖ Nenhum caractere suspeito encontrado neste teste")
                
            # An√°lise de caracteres
            print("\nüîç AN√ÅLISE DE CARACTERES (primeiros 20):")
            for i, char in enumerate(content[:20]):
                ascii_val = ord(char)
                status = "OK" if 32 <= ascii_val <= 126 else "SUSPEITO"
                print(f"  [{i}]: '{char}' (ASCII: {ascii_val}) - {status}")
        
        # Teste com prompt SQL
        print("\nüß™ TESTE COM PROMPT SQL")
        print("-" * 30)
        
        prompt_sql = """Gere SQL para: total faturado
        
Tabela: public.pedidosvenda (pv)
Campos: pedi_tota, pedi_empr

Resposta:"""
        
        print("Enviando prompt SQL...")
        resposta_sql = llm.invoke(prompt_sql)
        
        if hasattr(resposta_sql, 'content'):
            sql_content = resposta_sql.content
            print(f"SQL gerado: '{sql_content}'")
            
            # Verificar se √© SQL v√°lido ou corrompido
            if any(suspeito in sql_content.lower() for suspeito in suspeitos):
                print("‚ùå SQL CORROMPIDO DETECTADO!")
                print("   O LLM est√° definitivamente gerando texto corrompido.")
            elif 'select' in sql_content.lower():
                print("‚úÖ SQL aparentemente v√°lido gerado")
            else:
                print("‚ö†Ô∏è Resposta n√£o parece ser SQL")
                
    except ImportError as e:
        print(f"‚ùå Erro ao importar langchain_openai: {e}")
        print("   Verifique se a biblioteca est√° instalada.")
    except Exception as e:
        print(f"‚ùå Erro ao testar LLM: {e}")
        print("   Isso pode explicar por que o LLM gera respostas corrompidas.")

def analisar_logs_sistema():
    """Analisa poss√≠veis causas do problema"""
    print("\nüîç AN√ÅLISE DE POSS√çVEIS CAUSAS")
    print("=" * 50)
    
    causas_possiveis = [
        "1. OPENAI_API_KEY inv√°lida ou expirada",
        "2. Problema de rede/conectividade com OpenAI",
        "3. Modelo LLM configurado incorretamente",
        "4. Problema de encoding na comunica√ß√£o",
        "5. LLM usando modelo local/mock que gera texto aleat√≥rio",
        "6. Limite de tokens/rate limit atingido",
        "7. Vers√£o incompat√≠vel da biblioteca langchain"
    ]
    
    for causa in causas_possiveis:
        print(f"  {causa}")
    
    print("\nüí° RECOMENDA√á√ïES:")
    print("  ‚Ä¢ Verificar se OPENAI_API_KEY est√° correta")
    print("  ‚Ä¢ Testar conectividade com OpenAI")
    print("  ‚Ä¢ Verificar logs de erro do Django")
    print("  ‚Ä¢ Considerar usar modelo local como fallback")

if __name__ == "__main__":
    testar_llm_simples()
    analisar_logs_sistema()
